Backend - command line tools for interacting with MTurk API and creating HITs in MTurk

RUN ONCE:
	load_data_to_db.py
		populates langugages table in database
		loads data from wikilanguages-pipeline into database to vocabulary and dictionary tables respectively

	psql -h localhost -d tweets_es -f ../db/parallel_nottext.sql                
	

	    prepare_images_files.py
			generates images descriptor files for all vocabulary and dictionary items

		java  -Djava.awt.headless=true generateimages test_sentences.txt test_segments.txt images
		#java  -Djava.awt.headless=true generateimages test_sentences.txt test_segments.txt ../test_images
		#java  -Djava.awt.headless=true generateimages pilot_sentences.txt pilot_segments.txt ../pilot_images
			renders PNG files
			10MB of images per 1000 words+sentences

	generate_10sentences_hits.py
		creates hittypes in MTurk and populates hittypes table in database
		generates batches of vocabulary HITs in database based on vocabulary data
		populates hits table and voc_hits_data tables in database

	add_10sentences_hits_to_mturk.py
		generates HITs in MTurk and reference them to batches of words in database
		creates HITs in MTurk for every HIT in datbase with empty mturk_hit_id column
		updates hits table with non-empty mturk_hit_id value
		


LAUNCHD

	http://blog.mattbrock.co.uk/2010/02/25/moving-from-cron-to-launchd-on-mac-os-x-server/
	http://www.splinter.com.au/using-launchd-to-run-a-script-every-5-mins-on/
	
	sudo cp tweets_es.plist /Library/LaunchDaemons/
	
	launchctl load /Library/LaunchDaemons/tweets_es.plist
	
CONTINUOUS:

python multi_test.py
	get all assignments from MTurk

python buffer_update.py
	sync them with main database and load into tables by type of HIT

psql -h localhost -d tweets_es -f ../db/10sentences_to_similar.sql  
     genereate QC HITs from all completed 10 sentences assignments

python generate_similar_hits.py
	generate QC HITs batches
	

python add_similar_hits_to_mturk.py
	load batches/HITs into MTurk


#psql -h localhost -d mturk -f ../db/grade_qc.sql
	grade all QC hits automatically

#python process_qc.py
	process all graded QC HITs and pay/reject work done so far (and redo all messed up (low quality) work)

#level_qc_hits.py
	if similar HITs somehow still have less than 3 correctly done assignments, increment number of assignments manually

#psql -h localhost -d mturk -f ../db/update_10sentences_from_qc.sql
	load good quality data based on QC evaluations into original table - e.g. tweets/tensentences_hits_data(results)